---
title: "Why I'm Switching to Polars Away From Pandas"
subtitle: "Benchmarking Polars vs Pandas"
author: "Russ Zaliznyak <[rzaliznyak@gmail.com](mailto:rzaliznyak@gmail.com)>"
date: "2025-10-14"
execute: 
  echo: false
format: 
  html: 
    toc: true
    toc-expand: true
    toc-indent: true
---

# Introduction

Data scientists have long relied on Pandas for handling tabular data, but as datasets grow, its single-threaded design and memory overhead start to limit performance. Tasks that once took seconds can now take minutes, making large-scale analysis cumbersome.

That’s what led me to explore Polars — a Rust-based, multi-threaded DataFrame library built on Apache Arrow. It promises faster computation and better scalability out of the box. To see how it performs in practice, I benchmarked Polars against Pandas on common data operations. The results convinced me it’s time to switch.


::: {.callout-note collapse="true" title="System & Environment"}
- All benchmarks were run on a **16-inch MacBook Pro (2021)** equipped with an **Apple M1 Max** chip and **64 GB of unified memory**, running **macOS Sonoma 14.7**.  <br><br>
- Tests used **Python 3.10.9**, with **Pandas 1.5.3** and **Polars 1.33.0** installed.
:::


::: {.callout-note collapse="true" title="Methodology"}
I benchmarked **Polars** and **Pandas** across common DataFrame operations using both real and simulated data. <br><br>
- For **read** and **write** tests, I used a **10 MB CSV dataset** from the [Frictionless Data repository](https://raw.githubusercontent.com/frictionlessdata/datasets/main/files/csv/10mb.csv).  <br>
- For **constructor**, **pivot**, **join**, and **filter** benchmarks, I generated a **simulated dataset** to ensure consistent structure and reproducibility.  <br>
- Each operation was executed **5,000 times** per library, and the **mean execution time** was recorded.  
:::


# Read | Write 

[Download **10 mb file**: https://raw.githubusercontent.com/frictionlessdata/datasets/main/files/csv/10mb.csv](https://raw.githubusercontent.com/frictionlessdata/datasets/main/files/csv/10mb.csv){target=_blank}

```{python}
#| echo: false
#| code-fold: false
from numpy import mean, diff, array,std,percentile
from timeit import default_timer
import requests

file_location = "./data/10mb.csv"

# SHAPE OF CSV (9453, 9)
url = "https://raw.githubusercontent.com/frictionlessdata/datasets/main/files/csv/10mb.csv"
with open(file_location, "wb") as f:
    f.write(requests.get(url).content)
```




```{python}
#| code-summary: "Pandas read_csv()"
#| echo: false
#| code-fold: false
NUMBER_SIMULATIONS = int(5e3)
import pandas as pd

all_panda_read_times = []
all_panda_read_times = [default_timer()]
for j in range(NUMBER_SIMULATIONS):
    df_pandas = pd.read_csv(file_location)
    all_panda_read_times.append(default_timer())

all_panda_read_times = diff(all_panda_read_times)

panda_mean_run_time = mean(all_panda_read_times)
```

```{python}
#| code-summary: "Polars read_csv()"
#| echo: false
#| code-fold: false
import polars as pl

all_polar_read_times = [default_timer()]
for j in range(NUMBER_SIMULATIONS):
    df_polars =pl.read_csv(file_location)
    all_polar_read_times.append(default_timer())

all_polar_read_times = diff(all_polar_read_times)
polar_mean_run_time = mean(all_polar_read_times)

```

```{python}
#| code-summary: "Pandas to_csv()"
#| echo: false
#| code-fold: false
import pandas as pd

all_panda_write_times = [default_timer()]
for j in range(NUMBER_SIMULATIONS):
    df_pandas.to_csv("temp.csv")
    all_panda_write_times.append(default_timer())

all_panda_write_times = diff(all_panda_write_times)
panda_mean_write_time = mean(all_panda_write_times)


```

```{python}
#| code-summary: "Polars write_csv()"
#| echo: false
#| code-fold: false
import polars as pl

all_polar_write_times = [default_timer()]
for j in range(NUMBER_SIMULATIONS):
    df_polars.write_csv("temp.csv")
    all_polar_write_times.append(default_timer())

all_polar_write_times = diff(all_polar_write_times)
polar_write_run_time = mean(all_polar_write_times)

```


```{python}
#| code-summary: "Read Comparison"
#| echo: false
#| code-fold: false

from numpy import array
import plotly.figure_factory as ff

index_samples = array(all_panda_read_times)/array(all_polar_read_times)
speed_increase_mean = mean(index_samples)
speed_increase_std = std(index_samples)

fig = ff.create_distplot(
    [index_samples],
    group_labels=["index_samples"],  
    show_hist=False,              
    show_rug=False,          
)

fig.update_layout(showlegend=False,height = 300)
pass
#fig.show()


```


```{python}
#| code-summary: "Write Comparison"
#| echo: false
#| code-fold: true
index_samples_write = array(all_panda_write_times)/array(all_polar_write_times)
speed_increase_mean_write = mean(index_samples_write)
speed_increase_std_write = std(index_samples_write)

fig = ff.create_distplot(
    [index_samples_write],
    group_labels=["index_samples"],  
    show_hist=False,              
    show_rug=False,          
)

fig.update_layout(showlegend=False, height = 300)
pass
#fig.show()


```

| Metric       | Mean Speed Increase | Standard Error |
|--------------|---------------------|--------------------|
| Read         | `{python} f"{speed_increase_mean:.2f}"`         | `{python} f"{speed_increase_std/NUMBER_SIMULATIONS**0.5:.2f}"`          |
| Write        | `{python} f"{speed_increase_mean_write:.2f}"`   | `{python} f"{speed_increase_std_write/NUMBER_SIMULATIONS**0.5:.2f}"`    |


**Summary**: Polars can read and write CSVs many times faster than Pandas while using less memory and CPU per operation, making it ideal for medium-to-large datasets or repeated batch processing.

# Construct | Pivot

```{python}
#| code-summary: "Simulate Dataset"
#| echo: true
#| code-fold: true
import numpy as np
import string
import random

N = int(5e6)

def random_str(n=5):
    return ''.join(random.choices(string.ascii_letters, k=n))

data = {
    "id": np.arange(N),
    "value": np.random.randint(0, 1000, size=N),
    "flag": np.random.choice([True, False], size=N),
    "category": np.random.choice(["A", "B", "C", "D"], size=N),
    "score": np.random.randn(N),
    "year": np.random.randint(2000, 2025, size=N),
    "name": [random_str(5) for _ in range(N)],   # random short strings
    "ratio": np.random.random(size=N),
    "count": np.random.randint(1, 100, size=N),
}
```


```{python}
#| code-summary: "Pandas DataFrame()"
#| echo: false
#| code-fold: false
import pandas as pd

all_panda_constructor_times = [default_timer()]
for j in range(NUMBER_SIMULATIONS):
    df_pandas = pd.DataFrame.from_dict(data)
    all_panda_constructor_times.append(default_timer())

all_panda_constructor_times = diff(all_panda_constructor_times)
panda_constructor_mean_time = mean(all_panda_constructor_times)

```


```{python}
#| code-summary: "Polars DataFrame()"
#| echo: false
#| code-fold: false
import polars as pol

all_polar_constructor_times = [default_timer()]
for j in range(NUMBER_SIMULATIONS):
    df_polars = pl.DataFrame(data)
    all_polar_constructor_times.append(default_timer())

all_polar_constructor_times = diff(all_polar_constructor_times)
polar_constructor_mean_time = mean(all_polar_constructor_times)

```



```{python}
#| code-summary: "Constructor Comparison"
#| echo: false
#| code-fold: false
index_samples_constructor = array(all_polar_constructor_times)/array(all_panda_constructor_times)
speed_increase_mean_constructor = mean(index_samples_constructor)
speed_increase_std_constructor = std(index_samples_constructor)

fig = ff.create_distplot(
    [index_samples_constructor],
    group_labels=["index_samples"],  
    show_hist=False,              
    show_rug=False,          
)

fig.update_layout(showlegend=False,height = 300)
#fig.show()
pass


```




```{python}
#| code-summary: "Pandas group_by()"
#| echo: false
#| code-fold: false
import pandas as pd

all_panda_groupby_times = [default_timer()]
for j in range(NUMBER_SIMULATIONS):
    result = (
    df_pandas.groupby("category")
      .agg(
          count=("score", "size"),   # count(*) number of rows
          avg_score=("score", "mean")
      )
      .reset_index()
)
    all_panda_groupby_times.append(default_timer())

all_panda_groupby_times = diff(all_panda_groupby_times)
panda_groupby_mean_time = mean(all_panda_groupby_times)

```




```{python}
#| code-summary: "Polars group_by()"
#| echo: false
#| code-fold: false
import polars as pl

all_polar_groupby_times = [default_timer()]
for j in range(NUMBER_SIMULATIONS):
    result_polars = (
    df_polars.group_by("category")
      .agg([
          pl.len().alias("count"),         # count(*) number of rows
          pl.col("score").mean().alias("avg_score")
      ])
)
    all_polar_groupby_times.append(default_timer())

all_polar_groupby_times = diff(all_polar_groupby_times)
polar_groupby_mean_time = mean(all_polar_groupby_times)

```




```{python}
#| code-summary: "GroupBy Comparison"
#| echo: false
#| code-fold: false
index_samples_groupby = array(all_panda_groupby_times)/array(all_polar_groupby_times)
speed_increase_mean_groupby = mean(index_samples_groupby)
speed_increase_std_groupby = std(index_samples_groupby)

fig = ff.create_distplot(
    [index_samples_groupby],
    group_labels=["index_samples"],  
    show_hist=False,              
    show_rug=False,          
)

fig.update_layout(showlegend=False,height = 300)
#fig.show()
pass

```


| Metric       | Mean Speed Increase | Standard Error |
|--------------|---------------------|--------------------|
| Constructor  | `{python} f"{speed_increase_mean_constructor:.2f}"` | `{python} f"{speed_increase_std_constructor/NUMBER_SIMULATIONS**0.5:.2f}"` |
| Group By         | `{python} f"{speed_increase_mean_groupby:.2f}"`         | `{python} f"{speed_increase_std_groupby/NUMBER_SIMULATIONS**0.5:.2f}"`          |



**Summary**: Polars constructs and reshapes large datasets more quickly and efficiently by taking full advantage of multi-core CPUs and low-level optimizations that Pandas lacks


# Joins | Filter











```{python}
#| code-summary: "Pandas inner join"
#| echo: false
#| code-fold: false
import pandas as pd


all_panda_join_times = [default_timer()]
for j in range(NUMBER_SIMULATIONS):
    result_join = result.merge(result, on="category", how="inner")
    all_panda_join_times.append(default_timer())

all_panda_join_times = diff(all_panda_join_times)
panda_join_mean_time = mean(all_panda_join_times)

```




```{python}
#| code-summary: "Polars Inner Join"
#| echo: false
#| code-fold: false
import polars as pl

all_polar_join_times = [default_timer()]
for j in range(NUMBER_SIMULATIONS):
    result_polars_join = result_polars.join(result_polars, on="category", how="inner")
    all_polar_join_times.append(default_timer())

all_polar_join_times = diff(all_polar_join_times)
polar_join_mean_time = mean(all_polar_join_times)

```





```{python}
#| code-summary: "Inner Join Comparison"
#| echo: false
#| code-fold: false
index_samples_join = array(all_panda_join_times) / array(all_polar_join_times)
speed_increase_mean_join = mean(index_samples_join)
speed_increase_std_join = std(index_samples_join)

fig = ff.create_distplot(
    [index_samples_join],
    group_labels=["index_samples"],
    show_hist=False,
    show_rug=False,
)

fig.update_layout(showlegend=False, height=300)
pass
#fig.show()

```






```{python}
#| code-summary: "Pandas union"
#| echo: false
#| code-fold: false
import pandas as pd


all_panda_union_times = [default_timer()]
for j in range(NUMBER_SIMULATIONS):
    result_union = pd.concat([result, result], ignore_index=True)
    all_panda_union_times.append(default_timer())

all_panda_union_times = diff(all_panda_union_times)
panda_union_mean_time = mean(all_panda_union_times)

```




```{python}
#| code-summary: "Polars Inner Join"
#| echo: false
#| code-fold: false
import polars as pl

all_polar_union_times = [default_timer()]
for j in range(NUMBER_SIMULATIONS):
    result_polars_union = pl.concat([result_polars, result_polars])
    all_polar_union_times.append(default_timer())

all_polar_union_times = diff(all_polar_union_times)
polar_union_mean_time = mean(all_polar_union_times)

```




```{python}
#| code-summary: "Calc Comparison"
#| echo: false
#| code-fold: false
index_samples_union = array(all_panda_union_times) / array(all_polar_union_times)
speed_increase_mean_union = mean(index_samples_union)
speed_increase_std_union = std(index_samples_union)

fig = ff.create_distplot(
    [index_samples_union],
    group_labels=["index_samples"],
    show_hist=False,
    show_rug=False,
)

fig.update_layout(showlegend=False, height=300)
#fig.show()
pass
```






```{python}
#| code-summary: "Pandas Data Filtering"
#| echo: false
#| code-fold: false
import pandas as pd


all_panda_filter_times = [default_timer()]
for j in range(NUMBER_SIMULATIONS):
    result_filter = result[(result["avg_score"] > 10) | (result["count"] < 3)]
    all_panda_filter_times.append(default_timer())

all_panda_filter_times = diff(all_panda_filter_times)
panda_filter_mean_time = mean(all_panda_filter_times)

```




```{python}
#| code-summary: "Polars Data Filtering"
#| echo: false
#| code-fold: false
import polars as pl

all_polar_filter_times = [default_timer()]
for j in range(NUMBER_SIMULATIONS):


    result_polars_filter = result_polars.filter(
    (pl.col("avg_score") > 10) | (pl.col("count") < 3)
)
    all_polar_filter_times.append(default_timer())

all_polar_filter_times = diff(all_polar_filter_times)
polar_filter_mean_time = mean(all_polar_filter_times)

```





```{python}
#| code-summary: "filter Comparison"
#| echo: false
#| code-fold: true
index_samples_filter = array(all_panda_filter_times) / array(all_polar_filter_times)
speed_increase_mean_filter = mean(index_samples_filter)
speed_increase_std_filter = std(index_samples_filter)

fig = ff.create_distplot(
    [index_samples_filter],
    group_labels=["index_samples"],
    show_hist=False,
    show_rug=False,
)

fig.update_layout(showlegend=False, height=300)
pass
#fig.show()

```



| Metric       | Mean Speed Increase | Standard Error |
|--------------|---------------------|--------------------|
| Inner Join        | `{python} f"{speed_increase_mean_join:.2f}"`   | `{python} f"{speed_increase_std_join/NUMBER_SIMULATIONS**0.5:.2f}"`    |
| Union All  | `{python} f"{speed_increase_mean_union:.2f}"` | `{python} f"{speed_increase_std_union/NUMBER_SIMULATIONS**0.5:.2f}"` |
| Filter  | `{python} f"{speed_increase_mean_filter:.2f}"` | `{python} f"{speed_increase_std_filter/NUMBER_SIMULATIONS**0.5:.2f}"` |

<br>

**Summary**: Polars handles joins and filters faster and more efficiently by parallelizing work and minimizing Python-level overhead — making it ideal for high-volume or complex data transformations.

# Conclusion

As a data scientist, my goal is to explore and transform data quickly without fighting performance limits. Polars delivers that speed and efficiency out of the box — it loads data faster, executes transformations in parallel, and scales as datasets grow.

While Pandas remains great for lightweight analysis, Polars is simply better suited for large-scale workflows where time and memory efficiency matter. After benchmarking both, I’m convinced: Polars lets me spend less time waiting and more time analyzing.

# Code Comparisons

::: {.table-responsive}
| Operation | **Pandas Example** | **Polars Example** |
|------------|--------------------|--------------------|
| **Read CSV** | `df = pd.read_csv("data.csv")` | `df = pl.read_csv("data.csv")` |
| **Write CSV** | `df.to_csv("out.csv")` | `df.write_csv("out.csv")` |
| **Constructor** | `df = pd.DataFrame(data)` | `df = pl.DataFrame(data)` |
| **Group By** | `df.groupby("col")["value"].mean()` | `df.groupby("col").agg(pl.col("value").mean())` |
| **Inner Join** | `df1.merge(df2, on="id", how="inner")` | `df1.join(df2, on="id", how="inner")` |
| **Union All** | `pd.concat([df1, df2], ignore_index=True)` | `df1.vstack(df2)` |
| **Filter** | `df[df["value"] > 10]` | `df.filter(pl.col("value") > 10)` |
:::
